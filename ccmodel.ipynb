{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "## Homework 6: Putting it all together \n",
    "### Associated lectures: All material till lecture 13 \n",


    "\n",
    "**Due date: Monday, November 15, 2021 at 11:59pm**"

   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "- [Submission instructions](#si)\n",
    "- [Understanding the problem](#1)\n",
    "- [Data splitting](#2)\n",
    "- [EDA](#3)\n",
    "- (Optional) [Feature engineering](#4)\n",
    "- [Preprocessing and transformations](#5)\n",
    "- [Baseline model](#6)\n",
    "- [Linear models](#7)\n",
    "- [Different classifiers](#8)\n",
    "- (Optional) [Feature selection](#9)\n",
    "- [Hyperparameter optimization](#10)\n",
    "- [Interpretation and feature importances](#11)\n",
    "- [Results on the test set](#12)\n",
    "- (Optional) [Explaining predictions](#13)\n",
    "- [Summary of the results](#14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330/blob/master/docs/homework_instructions.md). \n",
    "\n",
    "**You may work on this homework in a group and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"in\"></a>\n",
    "<hr>\n",
    "\n",
    "At this point we are at the end of supervised machine learning part of the course. So in this homework, you will be working on an open-ended mini-project, where you will put all the different things you have learned so far together to solve an interesting problem.\n",
    "\n",
    "A few notes and tips when you work on this mini-project: \n",
    "\n",
    "#### Tips\n",
    "\n",
    "1. This mini-project is open-ended, and while working on it, there might be some situations where you'll have to use your own judgment and make your own decisions (as you would be doing when you work as a data scientist). Make sure you explain your decisions whenever necessary. \n",
    "2. **Do not include everything you ever tried in your submission** -- it's fine just to have your final code. That said, your code should be reproducible and well-documented. For example, if you chose your hyperparameters based on some hyperparameter optimization experiment, you should leave in the code for that experiment so that someone else could re-run it and obtain the same hyperparameters, rather than mysteriously just setting the hyperparameters to some (carefully chosen) values in your code. \n",
    "3. If you realize that you are repeating a lot of code try to organize it in functions. Clear presentation of your code, experiments, and results is the key to be successful in this lab. You may use code from lecture notes or previous lab solutions with appropriate attributions. \n",
    "4. If you are having trouble running models on your laptop because of the size of the dataset, you can create your train/test split in such a way that you have less data in the train split. If you end up doing this, please write a note to the grader in the submission explaining why you are doing it.  \n",
    "\n",
    "#### Assessment\n",
    "\n",
    "We plan to grade fairly and leniently. We don't have some secret target score that you need to achieve to get a good grade. **You'll be assessed on demonstration of mastery of course topics, clear presentation, and the quality of your analysis and results.** For example, if you just have a bunch of code and no text or figures, that's not good. If you do a bunch of sane things and get a lower accuracy than your friend, don't sweat it.\n",
    "\n",
    "#### A final note\n",
    "\n",
    "Finally, this style of this \"project\" question is different from other assignments. It'll be up to you to decide when you're \"done\" -- in fact, this is one of the hardest parts of real projects. But please don't spend WAY too much time on this... perhaps \"a few hours\" (4-10 hours) is a good guideline for a typical submission. Of course if you're having fun you're welcome to spend as much time as you want! But, if so, try not to do it out of perfectionism or getting the best possible grade. Do it because you're learning and enjoying it. Students from the past cohorts have found such kind of labs useful and fun and I hope you enjoy it as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the problem <a name=\"1\"></a>\n",
    "<hr>\n",
    "rubric={points:4}\n",
    "\n",
    "In this mini project, you will be working on a classification problem of predicting whether a credit card client will default or not. \n",
    "For this problem, you will use [Default of Credit Card Clients Dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). In this data set, there are 30,000 examples and 24 features, and the goal is to estimate whether a person will default (fail to pay) their credit card bills; this column is labeled \"default.payment.next.month\" in the data. The rest of the columns can be used as features. You may take some ideas and compare your results with [the associated research paper](https://www.sciencedirect.com/science/article/pii/S0957417407006719), which is available through [the UBC library](https://www.library.ubc.ca/). \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Spend some time understanding the problem and what each feature means. You can find this information in the documentation on [the dataset page on Kaggle](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). Write a few sentences on your initial thoughts on the problem and the dataset. \n",
    "2. Download the dataset and read it as a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"UCI_Credit_Card.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a data set with data about customers and their credit card habits. It will be interesting to see what factors affect credit card defaulting the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data splitting <a name=\"2\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the data into train and test portions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16395</th>\n",
       "      <td>16396</td>\n",
       "      <td>320000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19370.0</td>\n",
       "      <td>10155.0</td>\n",
       "      <td>3788.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5018.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7013.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21448</th>\n",
       "      <td>21449</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>171244.0</td>\n",
       "      <td>150897.0</td>\n",
       "      <td>117870.0</td>\n",
       "      <td>612.0</td>\n",
       "      <td>87426.0</td>\n",
       "      <td>130007.0</td>\n",
       "      <td>3018.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>51663.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20034</th>\n",
       "      <td>20035</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25755</th>\n",
       "      <td>25756</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>103058.0</td>\n",
       "      <td>71095.0</td>\n",
       "      <td>47379.0</td>\n",
       "      <td>3706.0</td>\n",
       "      <td>5502.0</td>\n",
       "      <td>4204.0</td>\n",
       "      <td>3017.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>1702.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>1439</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>27585.0</td>\n",
       "      <td>27910.0</td>\n",
       "      <td>27380.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28636</th>\n",
       "      <td>28637</td>\n",
       "      <td>380000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14291.0</td>\n",
       "      <td>15949.0</td>\n",
       "      <td>15556.0</td>\n",
       "      <td>1204.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1242.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17730</th>\n",
       "      <td>17731</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>2580.0</td>\n",
       "      <td>2580.0</td>\n",
       "      <td>6941.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2580.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6941.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28030</th>\n",
       "      <td>28031</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>39420.0</td>\n",
       "      <td>8824.0</td>\n",
       "      <td>9009.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1530.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15725</th>\n",
       "      <td>15726</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>30841.0</td>\n",
       "      <td>29434.0</td>\n",
       "      <td>16565.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1765.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19966</th>\n",
       "      <td>19967</td>\n",
       "      <td>370000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>4246.0</td>\n",
       "      <td>3737.0</td>\n",
       "      <td>2290.0</td>\n",
       "      <td>25794.0</td>\n",
       "      <td>3007.0</td>\n",
       "      <td>4305.0</td>\n",
       "      <td>3737.0</td>\n",
       "      <td>2290.0</td>\n",
       "      <td>22259.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  \\\n",
       "16395  16396   320000.0    2          1         2   36      0      0      0   \n",
       "21448  21449   440000.0    2          1         2   30     -1     -1     -1   \n",
       "20034  20035   160000.0    2          3         1   44     -2     -2     -2   \n",
       "25755  25756   120000.0    2          2         1   30      0      0      0   \n",
       "1438    1439    50000.0    1          2         2   54      1      2      0   \n",
       "...      ...        ...  ...        ...       ...  ...    ...    ...    ...   \n",
       "28636  28637   380000.0    2          2         1   37      0      0      0   \n",
       "17730  17731   360000.0    2          1         1   54      1     -2     -2   \n",
       "28030  28031    50000.0    2          3         1   29      0      0      0   \n",
       "15725  15726    30000.0    2          2         2   21      0      0      0   \n",
       "19966  19967   370000.0    2          1         1   36     -2     -2     -2   \n",
       "\n",
       "       PAY_4  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "16395      0  ...    19370.0    10155.0     3788.0    5000.0    5018.0   \n",
       "21448      0  ...   171244.0   150897.0   117870.0     612.0   87426.0   \n",
       "20034     -2  ...      -18.0      -18.0      -18.0       0.0       0.0   \n",
       "25755      0  ...   103058.0    71095.0    47379.0    3706.0    5502.0   \n",
       "1438       0  ...    27585.0    27910.0    27380.0       0.0    1400.0   \n",
       "...      ...  ...        ...        ...        ...       ...       ...   \n",
       "28636      0  ...    14291.0    15949.0    15556.0    1204.0    1300.0   \n",
       "17730     -1  ...     2580.0     2580.0     6941.0       0.0       0.0   \n",
       "28030      0  ...    39420.0     8824.0     9009.0    2002.0    2000.0   \n",
       "15725      0  ...    30841.0    29434.0    16565.0    2000.0    2000.0   \n",
       "19966     -2  ...     4246.0     3737.0     2290.0   25794.0    3007.0   \n",
       "\n",
       "       PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "16395    1000.0    3000.0       0.0    7013.0                           0  \n",
       "21448  130007.0    3018.0   15000.0   51663.0                           0  \n",
       "20034       0.0       0.0       0.0       0.0                           0  \n",
       "25755    4204.0    3017.0    2005.0    1702.0                           0  \n",
       "1438     1200.0    1500.0    1000.0    1500.0                           0  \n",
       "...         ...       ...       ...       ...                         ...  \n",
       "28636    1242.0    1900.0       0.0    3000.0                           0  \n",
       "17730    2580.0       0.0    6941.0       0.0                           0  \n",
       "28030    1530.0    1000.0     327.0     329.0                           1  \n",
       "15725    2000.0    1765.0     331.0       0.0                           0  \n",
       "19966    4305.0    3737.0    2290.0   22259.0                           0  \n",
       "\n",
       "[21000 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=123)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA <a name=\"3\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Perform exploratory data analysis on the train set.\n",
    "2. Include at least two summary statistics and two visualizations that you find useful, and accompany each one with a sentence explaining it.\n",
    "3. Summarize your initial observations about the data. \n",
    "4. Pick appropriate metric/metrics for assessment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 21000 entries, 16395 to 19966\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   ID                          21000 non-null  int64  \n",
      " 1   LIMIT_BAL                   21000 non-null  float64\n",
      " 2   SEX                         21000 non-null  int64  \n",
      " 3   EDUCATION                   21000 non-null  int64  \n",
      " 4   MARRIAGE                    21000 non-null  int64  \n",
      " 5   AGE                         21000 non-null  int64  \n",
      " 6   PAY_0                       21000 non-null  int64  \n",
      " 7   PAY_2                       21000 non-null  int64  \n",
      " 8   PAY_3                       21000 non-null  int64  \n",
      " 9   PAY_4                       21000 non-null  int64  \n",
      " 10  PAY_5                       21000 non-null  int64  \n",
      " 11  PAY_6                       21000 non-null  int64  \n",
      " 12  BILL_AMT1                   21000 non-null  float64\n",
      " 13  BILL_AMT2                   21000 non-null  float64\n",
      " 14  BILL_AMT3                   21000 non-null  float64\n",
      " 15  BILL_AMT4                   21000 non-null  float64\n",
      " 16  BILL_AMT5                   21000 non-null  float64\n",
      " 17  BILL_AMT6                   21000 non-null  float64\n",
      " 18  PAY_AMT1                    21000 non-null  float64\n",
      " 19  PAY_AMT2                    21000 non-null  float64\n",
      " 20  PAY_AMT3                    21000 non-null  float64\n",
      " 21  PAY_AMT4                    21000 non-null  float64\n",
      " 22  PAY_AMT5                    21000 non-null  float64\n",
      " 23  PAY_AMT6                    21000 non-null  float64\n",
      " 24  default.payment.next.month  21000 non-null  int64  \n",
      "dtypes: float64(13), int64(12)\n",
      "memory usage: 4.2 MB\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.info()\n",
    "train_df.sort_index()\n",
    "num_cats = [\"LIMIT_BAL\",\"SEX\",\"EDUCATION\",\"MARRIAGE\",\"AGE\"]\n",
    "for column in num_cats:\n",
    "    title = \"Histogram of \" + column + \" by target class\"\n",
    "    train_df[column].plot(kind='hist', alpha=0.5, bins=30)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = (\n",
    "    train_df.drop(columns=[\"default.payment.next.month\"]),\n",
    "    train_df[\"default.payment.next.month\"],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    test_df.drop(columns=[\"default.payment.next.month\"]),\n",
    "    test_df[\"default.payment.next.month\"],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction that default:\n",
      " 0    0.776762\n",
      "1    0.223238\n",
      "Name: default.payment.next.month, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Fraction that default:\\n\",\n",
    "    train_df[\"default.payment.next.month\"].value_counts(normalize=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, make_scorer, recall_score\n",
    "\n",
    "custom_scorer = make_scorer(f1_score, average=\"macro\")\n",
    "scoring_metric = custom_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we see that all our feature values are numerical in value. I will omit the id, pay columns, bill amount columns and pay_amt columns for now and visualize the rest in histograms. Some of our features are also categorical such as sex, marriage and education so we will need transformations later. Both our AGE and LIMIT_BAL are right skewed with our values mostly being on the left. We will use macro-f1 as our scoring because we have class imbalance so accuracy will not be as telling of an indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) 4. Feature engineering <a name=\"4\"></a>\n",
    "<hr>\n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Carry out feature engineering. In other words, extract new features relevant for the problem and work with your new feature set in the following exercises. You may have to go back and forth between feature engineering and preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing and transformations <a name=\"5\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Identify different feature types and the transformations you would apply on each feature type. \n",
    "2. Define a column transformer, if necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1   LIMIT_BAL: scaling              \n",
    " 2   SEX: ohe                         \n",
    " 3   EDUCATION: OHE there is already ordinality in original data                    \n",
    " 4   MARRIAGE: ohe                  \n",
    " 5   AGE: scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"LIMIT_BAL\",\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_5\",\n",
    "    \"PAY_6\",\n",
    "    \"BILL_AMT1\",\n",
    "    \"BILL_AMT2\",\n",
    "    \"BILL_AMT3\",\n",
    "    \"BILL_AMT4\",\n",
    "    \"BILL_AMT5\",\n",
    "    \"BILL_AMT6\",\n",
    "    \"PAY_AMT1\",\n",
    "    \"PAY_AMT2\",\n",
    "    \"PAY_AMT3\",\n",
    "    \"PAY_AMT4\",\n",
    "    \"PAY_AMT5\",\n",
    "    \"PAY_AMT6\",\n",
    "    \"AGE\",\n",
    "]\n",
    "categorical_features = [\"MARRIAGE\",\"EDUCATION\"]\n",
    "binary_features = [\"SEX\"]\n",
    "drop_features = [\"ID\"]\n",
    "passthrough_features = []\n",
    "target = \"default.payment.next.month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (StandardScaler(),numeric_features),\n",
    "    (OneHotEncoder(drop=\"if_binary\"), binary_features),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    ")\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline model <a name=\"6\"></a>\n",
    "<hr>\n",
    "\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Try `scikit-learn`'s baseline model and report results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.041005</td>\n",
       "      <td>0.018002</td>\n",
       "      <td>0.498289</td>\n",
       "      <td>0.504337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.041959</td>\n",
       "      <td>0.017990</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.492925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.066002</td>\n",
       "      <td>0.016993</td>\n",
       "      <td>0.514027</td>\n",
       "      <td>0.508731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.036005</td>\n",
       "      <td>0.016058</td>\n",
       "      <td>0.503232</td>\n",
       "      <td>0.500657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.037086</td>\n",
       "      <td>0.017970</td>\n",
       "      <td>0.500127</td>\n",
       "      <td>0.498598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.041005    0.018002    0.498289     0.504337\n",
       "1  0.041959    0.017990    0.507400     0.492925\n",
       "2  0.066002    0.016993    0.514027     0.508731\n",
       "3  0.036005    0.016058    0.503232     0.500657\n",
       "4  0.037086    0.017970    0.500127     0.498598"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr = make_pipeline(\n",
    "    preprocessor, DummyClassifier(strategy=\"stratified\")\n",
    ")\n",
    "results[\"dummy\"] = cross_validate(\n",
    "    pipe_lr, X_train, y_train, return_train_score=True, scoring = custom_scorer \n",
    ")\n",
    "pd.DataFrame(results[\"dummy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Linear models <a name=\"7\"></a>\n",
    "<hr>\n",
    "rubric={points:12}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try logistic regression as a first real attempt. \n",
    "2. Carry out hyperparameter tuning to explore different values for the complexity hyperparameter `C`. \n",
    "3. Report validation scores along with standard deviation. \n",
    "4. Summarize your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.660994</td>\n",
       "      <td>0.022997</td>\n",
       "      <td>0.615845</td>\n",
       "      <td>0.626924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.750997</td>\n",
       "      <td>0.016991</td>\n",
       "      <td>0.621635</td>\n",
       "      <td>0.629409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.795013</td>\n",
       "      <td>0.018989</td>\n",
       "      <td>0.628525</td>\n",
       "      <td>0.626176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.554999</td>\n",
       "      <td>0.018997</td>\n",
       "      <td>0.628419</td>\n",
       "      <td>0.623648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.800006</td>\n",
       "      <td>0.035996</td>\n",
       "      <td>0.630111</td>\n",
       "      <td>0.631221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.660994    0.022997    0.615845     0.626924\n",
       "1  0.750997    0.016991    0.621635     0.629409\n",
       "2  0.795013    0.018989    0.628525     0.626176\n",
       "3  0.554999    0.018997    0.628419     0.623648\n",
       "4  0.800006    0.035996    0.630111     0.631221"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr = make_pipeline(\n",
    "    preprocessor, LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    ")\n",
    "results[\"logreg\"] = cross_validate(\n",
    "    pipe_lr, X_train, y_train, return_train_score=True, scoring = custom_scorer \n",
    ")\n",
    "pd.DataFrame(results[\"logreg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"logisticregression__C\": 2.0 ** np.arange(-5, 5),\n",
    "    \"logisticregression__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    pipe_lr,\n",
    "    param_grid,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    "    scoring = custom_scorer,\n",
    ")\n",
    "grid.fit(X_train, y_train);\n",
    "grid_df = pd.DataFrame(grid.cv_results_)[\n",
    "    [\n",
    "        \"mean_test_score\",\n",
    "        \"mean_train_score\",\n",
    "        \"param_logisticregression__C\",\n",
    "        \"param_logisticregression__class_weight\",\n",
    "        \"rank_test_score\",\n",
    "    ]\n",
    "]\n",
    "grid_df = grid_df.sort_values(by=\"mean_test_score\", ascending=False)\n",
    "grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 14\n",
      "fit_time       0.042134\n",
      "score_time     0.006967\n",
      "test_score     0.006161\n",
      "train_score    0.003041\n",
      "dtype: float64\n",
      "fit_time       0.287327\n",
      "score_time     0.021993\n",
      "test_score     0.627602\n",
      "train_score    0.629873\n",
      "dtype: float64\n",
      "   fit_time  score_time  test_score  train_score\n",
      "0  0.330006    0.019991    0.618541     0.629603\n",
      "1  0.264014    0.015989    0.623828     0.631320\n",
      "2  0.334813    0.020994    0.632352     0.628321\n",
      "3  0.242800    0.018993    0.631260     0.626040\n",
      "4  0.265002    0.033998    0.632027     0.634083\n"
     ]
    }
   ],
   "source": [
    "pipe_lr2 = make_pipeline(\n",
    "    preprocessor, LogisticRegression(max_iter=1000, class_weight=\"balanced\",C = 0.03125)\n",
    ")\n",
    "results[\"logreg (tuned)\"] = cross_validate(\n",
    "    pipe_lr2, X_train, y_train, return_train_score=True, scoring = custom_scorer \n",
    ")\n",
    "mean_scores = pd.DataFrame(results[\"logreg (tuned)\"]).mean()\n",
    "std_scores = pd.DataFrame(results[\"logreg (tuned)\"]).std()\n",
    "pd.DataFrame(results[\"logreg (tuned)\"])\n",
    "print( std_scores)\n",
    "print(mean_scores)\n",
    "print(pd.DataFrame(results[\"logreg (tuned)\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test and train scores are better than DummyClassifier model by around 0.12, which means the logistic regression model is better but there may be better models that we can use. Using optimized hyperparameters also helped the scores improve marginally. Ww may be underfitting as there is not much difference between train and test scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Different classifiers <a name=\"8\"></a>\n",
    "<hr>\n",
    "rubric={points:15}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Try at least 3 other models aside from logistic regression. At least one of these models should be a tree-based ensemble model (e.g., lgbm, random forest, xgboost). \n",
    "2. Summarize your results. Can you beat logistic regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = {\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1348, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 93, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1348, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 93, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1348, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 93, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1348, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 93, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1348, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 93, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1348, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 93, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1348, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 93, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1348, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 93, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1348, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 93, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 216, in __call__\n",
      "    return self._score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1123, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1261, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1348, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 93, in _check_targets\n",
      "    raise ValueError(\n",
      "ValueError: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ridge</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>logreg</th>\n",
       "      <th>logreg (tuned)</th>\n",
       "      <th>LGBM</th>\n",
       "      <th>dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.073601</td>\n",
       "      <td>6.515924</td>\n",
       "      <td>[0.6609938144683838, 0.7509973049163818, 0.795...</td>\n",
       "      <td>[0.33000636100769043, 0.26401400566101074, 0.3...</td>\n",
       "      <td>0.546242</td>\n",
       "      <td>[0.04100489616394043, 0.04195880889892578, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.022609</td>\n",
       "      <td>0.194511</td>\n",
       "      <td>[0.022997379302978516, 0.01699090003967285, 0....</td>\n",
       "      <td>[0.01999068260192871, 0.015989303588867188, 0....</td>\n",
       "      <td>0.035114</td>\n",
       "      <td>[0.018001556396484375, 0.017989635467529297, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_score</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.681940</td>\n",
       "      <td>[0.6158453731694608, 0.6216349263590519, 0.628...</td>\n",
       "      <td>[0.6185414311057977, 0.6238284121538838, 0.632...</td>\n",
       "      <td>0.684933</td>\n",
       "      <td>[0.4982889923477956, 0.5073998289432373, 0.514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_score</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999142</td>\n",
       "      <td>[0.6269241318547615, 0.6294089962807825, 0.626...</td>\n",
       "      <td>[0.6296032432901842, 0.6313197385583444, 0.628...</td>\n",
       "      <td>0.738843</td>\n",
       "      <td>[0.504337438109785, 0.49292482044606395, 0.508...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Ridge  Random Forest  \\\n",
       "fit_time     0.073601       6.515924   \n",
       "score_time   0.022609       0.194511   \n",
       "test_score        NaN       0.681940   \n",
       "train_score       NaN       0.999142   \n",
       "\n",
       "                                                        logreg  \\\n",
       "fit_time     [0.6609938144683838, 0.7509973049163818, 0.795...   \n",
       "score_time   [0.022997379302978516, 0.01699090003967285, 0....   \n",
       "test_score   [0.6158453731694608, 0.6216349263590519, 0.628...   \n",
       "train_score  [0.6269241318547615, 0.6294089962807825, 0.626...   \n",
       "\n",
       "                                                logreg (tuned)      LGBM  \\\n",
       "fit_time     [0.33000636100769043, 0.26401400566101074, 0.3...  0.546242   \n",
       "score_time   [0.01999068260192871, 0.015989303588867188, 0....  0.035114   \n",
       "test_score   [0.6185414311057977, 0.6238284121538838, 0.632...  0.684933   \n",
       "train_score  [0.6296032432901842, 0.6313197385583444, 0.628...  0.738843   \n",
       "\n",
       "                                                         dummy  \n",
       "fit_time     [0.04100489616394043, 0.04195880889892578, 0.0...  \n",
       "score_time   [0.018001556396484375, 0.017989635467529297, 0...  \n",
       "test_score   [0.4982889923477956, 0.5073998289432373, 0.514...  \n",
       "train_score  [0.504337438109785, 0.49292482044606395, 0.508...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for model in models:\n",
    "    pipe = make_pipeline(preprocessor, models[model])\n",
    "    results[model] = pd.DataFrame(cross_validate(pipe, X_train, y_train, return_train_score=True, scoring = custom_scorer )).mean()\n",
    "    \n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our results we can see that Random Forest and LGBM perform the best out of our models but our Random Forest Model may be overfitted as our train_score is a lot higher than our test_score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) 9. Feature selection <a name=\"9\"></a>\n",
    "<hr>\n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to select relevant features. You may try `RFECV` or forward selection. Do the results improve with feature selection? Summarize your results. If you see improvements in the results, keep feature selection in your pipeline. If not, you may abandon it in the next exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter optimization <a name=\"10\"></a>\n",
    "<hr>\n",
    "rubric={points:15}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to optimize hyperparameters for the models you've tried and summarize your results. You may pick one of the best performing models from the previous exercise and tune hyperparameters only for that model. You may use `sklearn`'s methods for hyperparameter optimization or fancier Bayesian optimization methods. \n",
    "  - [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)   \n",
    "  - [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "  - [scikit-optimize](https://github.com/scikit-optimize/scikit-optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will pick LGBM as my best performing model to perform hyperparameter optimization on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_lgbmclassifier__n_estimators</th>\n",
       "      <th>param_lgbmclassifier__learning_rate</th>\n",
       "      <th>param_lgbmclassifier__num_leaves</th>\n",
       "      <th>param_lgbmclassifier__max_depth</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.700390</td>\n",
       "      <td>0.687356</td>\n",
       "      <td>500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7</td>\n",
       "      <td>-5</td>\n",
       "      <td>3.169050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.700390</td>\n",
       "      <td>0.687356</td>\n",
       "      <td>500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.131531</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.694725</td>\n",
       "      <td>0.686947</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.129297</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.694725</td>\n",
       "      <td>0.686947</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>-5</td>\n",
       "      <td>1.123234</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.695621</td>\n",
       "      <td>0.686816</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1.098557</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.699790</td>\n",
       "      <td>0.686545</td>\n",
       "      <td>500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3.212270</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.723363</td>\n",
       "      <td>0.686452</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.466649</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.723363</td>\n",
       "      <td>0.686452</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>21</td>\n",
       "      <td>-5</td>\n",
       "      <td>1.440759</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.706182</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>21</td>\n",
       "      <td>-5</td>\n",
       "      <td>1.026436</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.706182</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.998854</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.710075</td>\n",
       "      <td>0.685644</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>14</td>\n",
       "      <td>-5</td>\n",
       "      <td>1.253935</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.710075</td>\n",
       "      <td>0.685644</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.344001</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.698862</td>\n",
       "      <td>0.685567</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>14</td>\n",
       "      <td>-5</td>\n",
       "      <td>0.854937</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.698862</td>\n",
       "      <td>0.685567</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.900863</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.689971</td>\n",
       "      <td>0.685468</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.775499</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.731399</td>\n",
       "      <td>0.685433</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3.236266</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.719022</td>\n",
       "      <td>0.685358</td>\n",
       "      <td>500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.995424</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.719022</td>\n",
       "      <td>0.685358</td>\n",
       "      <td>500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>14</td>\n",
       "      <td>-5</td>\n",
       "      <td>3.881981</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.729487</td>\n",
       "      <td>0.685355</td>\n",
       "      <td>500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>4.074039</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.716642</td>\n",
       "      <td>0.685278</td>\n",
       "      <td>500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>3.568991</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.738861</td>\n",
       "      <td>0.684750</td>\n",
       "      <td>500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.565319</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.738861</td>\n",
       "      <td>0.684750</td>\n",
       "      <td>500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>21</td>\n",
       "      <td>-5</td>\n",
       "      <td>4.922649</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.699060</td>\n",
       "      <td>0.684584</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0.719414</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.708112</td>\n",
       "      <td>0.684196</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1.146953</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.731827</td>\n",
       "      <td>0.683841</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.501403</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.731827</td>\n",
       "      <td>0.683841</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>-5</td>\n",
       "      <td>4.092425</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.694368</td>\n",
       "      <td>0.683816</td>\n",
       "      <td>100</td>\n",
       "      <td>0.03</td>\n",
       "      <td>21</td>\n",
       "      <td>-5</td>\n",
       "      <td>1.347827</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.694368</td>\n",
       "      <td>0.683816</td>\n",
       "      <td>100</td>\n",
       "      <td>0.03</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.301132</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.689936</td>\n",
       "      <td>0.683656</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>-5</td>\n",
       "      <td>0.630462</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.689936</td>\n",
       "      <td>0.683656</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.705538</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.706076</td>\n",
       "      <td>0.683595</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>0.806302</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.690269</td>\n",
       "      <td>0.683439</td>\n",
       "      <td>100</td>\n",
       "      <td>0.03</td>\n",
       "      <td>14</td>\n",
       "      <td>-5</td>\n",
       "      <td>1.236416</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.690269</td>\n",
       "      <td>0.683439</td>\n",
       "      <td>100</td>\n",
       "      <td>0.03</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.143029</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.692205</td>\n",
       "      <td>0.683072</td>\n",
       "      <td>100</td>\n",
       "      <td>0.03</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1.188575</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.685473</td>\n",
       "      <td>0.682875</td>\n",
       "      <td>100</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7</td>\n",
       "      <td>-5</td>\n",
       "      <td>1.179529</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.685473</td>\n",
       "      <td>0.682875</td>\n",
       "      <td>100</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.981587</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.685577</td>\n",
       "      <td>0.682801</td>\n",
       "      <td>100</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.975862</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.698175</td>\n",
       "      <td>0.682676</td>\n",
       "      <td>100</td>\n",
       "      <td>0.03</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>1.094443</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.717541</td>\n",
       "      <td>0.682383</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>1.192916</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.801926</td>\n",
       "      <td>0.681185</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.721466</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.801926</td>\n",
       "      <td>0.681185</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>14</td>\n",
       "      <td>-5</td>\n",
       "      <td>4.264700</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.786668</td>\n",
       "      <td>0.680004</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>3.686394</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.863683</td>\n",
       "      <td>0.678742</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>21</td>\n",
       "      <td>-5</td>\n",
       "      <td>4.857510</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.863683</td>\n",
       "      <td>0.678742</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>5.575354</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.824533</td>\n",
       "      <td>0.677515</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>4.472689</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.675635</td>\n",
       "      <td>0.668200</td>\n",
       "      <td>50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>0.730955</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.674019</td>\n",
       "      <td>0.668164</td>\n",
       "      <td>50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>21</td>\n",
       "      <td>-5</td>\n",
       "      <td>1.083035</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.674019</td>\n",
       "      <td>0.668164</td>\n",
       "      <td>50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.779873</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.668390</td>\n",
       "      <td>0.667937</td>\n",
       "      <td>50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.615090</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.668390</td>\n",
       "      <td>0.667937</td>\n",
       "      <td>50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7</td>\n",
       "      <td>-5</td>\n",
       "      <td>0.675093</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.668461</td>\n",
       "      <td>0.667587</td>\n",
       "      <td>50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.671027</td>\n",
       "      <td>0.666519</td>\n",
       "      <td>50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.775058</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.671027</td>\n",
       "      <td>0.666519</td>\n",
       "      <td>50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>14</td>\n",
       "      <td>-5</td>\n",
       "      <td>0.740573</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.670386</td>\n",
       "      <td>0.665994</td>\n",
       "      <td>50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0.685408</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_train_score  mean_test_score param_lgbmclassifier__n_estimators  \\\n",
       "33          0.700390         0.687356                                500   \n",
       "42          0.700390         0.687356                                500   \n",
       "12          0.694725         0.686947                                100   \n",
       "3           0.694725         0.686947                                100   \n",
       "21          0.695621         0.686816                                100   \n",
       "51          0.699790         0.686545                                500   \n",
       "14          0.723363         0.686452                                100   \n",
       "5           0.723363         0.686452                                100   \n",
       "2           0.706182         0.685802                                 50   \n",
       "11          0.706182         0.685802                                 50   \n",
       "4           0.710075         0.685644                                100   \n",
       "13          0.710075         0.685644                                100   \n",
       "1           0.698862         0.685567                                 50   \n",
       "10          0.698862         0.685567                                 50   \n",
       "18          0.689971         0.685468                                 50   \n",
       "24          0.731399         0.685433                                500   \n",
       "43          0.719022         0.685358                                500   \n",
       "34          0.719022         0.685358                                500   \n",
       "53          0.729487         0.685355                                500   \n",
       "52          0.716642         0.685278                                500   \n",
       "44          0.738861         0.684750                                500   \n",
       "35          0.738861         0.684750                                500   \n",
       "19          0.699060         0.684584                                 50   \n",
       "22          0.708112         0.684196                                100   \n",
       "15          0.731827         0.683841                                500   \n",
       "6           0.731827         0.683841                                500   \n",
       "32          0.694368         0.683816                                100   \n",
       "41          0.694368         0.683816                                100   \n",
       "0           0.689936         0.683656                                 50   \n",
       "9           0.689936         0.683656                                 50   \n",
       "20          0.706076         0.683595                                 50   \n",
       "31          0.690269         0.683439                                100   \n",
       "40          0.690269         0.683439                                100   \n",
       "49          0.692205         0.683072                                100   \n",
       "30          0.685473         0.682875                                100   \n",
       "39          0.685473         0.682875                                100   \n",
       "48          0.685577         0.682801                                100   \n",
       "50          0.698175         0.682676                                100   \n",
       "23          0.717541         0.682383                                100   \n",
       "16          0.801926         0.681185                                500   \n",
       "7           0.801926         0.681185                                500   \n",
       "25          0.786668         0.680004                                500   \n",
       "8           0.863683         0.678742                                500   \n",
       "17          0.863683         0.678742                                500   \n",
       "26          0.824533         0.677515                                500   \n",
       "47          0.675635         0.668200                                 50   \n",
       "29          0.674019         0.668164                                 50   \n",
       "38          0.674019         0.668164                                 50   \n",
       "36          0.668390         0.667937                                 50   \n",
       "27          0.668390         0.667937                                 50   \n",
       "45          0.668461         0.667587                                 50   \n",
       "37          0.671027         0.666519                                 50   \n",
       "28          0.671027         0.666519                                 50   \n",
       "46          0.670386         0.665994                                 50   \n",
       "\n",
       "   param_lgbmclassifier__learning_rate param_lgbmclassifier__num_leaves  \\\n",
       "33                                0.03                                7   \n",
       "42                                0.03                                7   \n",
       "12                                 0.1                                7   \n",
       "3                                  0.1                                7   \n",
       "21                                 0.1                                7   \n",
       "51                                0.03                                7   \n",
       "14                                 0.1                               21   \n",
       "5                                  0.1                               21   \n",
       "2                                  0.1                               21   \n",
       "11                                 0.1                               21   \n",
       "4                                  0.1                               14   \n",
       "13                                 0.1                               14   \n",
       "1                                  0.1                               14   \n",
       "10                                 0.1                               14   \n",
       "18                                 0.1                                7   \n",
       "24                                 0.1                                7   \n",
       "43                                0.03                               14   \n",
       "34                                0.03                               14   \n",
       "53                                0.03                               21   \n",
       "52                                0.03                               14   \n",
       "44                                0.03                               21   \n",
       "35                                0.03                               21   \n",
       "19                                 0.1                               14   \n",
       "22                                 0.1                               14   \n",
       "15                                 0.1                                7   \n",
       "6                                  0.1                                7   \n",
       "32                                0.03                               21   \n",
       "41                                0.03                               21   \n",
       "0                                  0.1                                7   \n",
       "9                                  0.1                                7   \n",
       "20                                 0.1                               21   \n",
       "31                                0.03                               14   \n",
       "40                                0.03                               14   \n",
       "49                                0.03                               14   \n",
       "30                                0.03                                7   \n",
       "39                                0.03                                7   \n",
       "48                                0.03                                7   \n",
       "50                                0.03                               21   \n",
       "23                                 0.1                               21   \n",
       "16                                 0.1                               14   \n",
       "7                                  0.1                               14   \n",
       "25                                 0.1                               14   \n",
       "8                                  0.1                               21   \n",
       "17                                 0.1                               21   \n",
       "26                                 0.1                               21   \n",
       "47                                0.03                               21   \n",
       "29                                0.03                               21   \n",
       "38                                0.03                               21   \n",
       "36                                0.03                                7   \n",
       "27                                0.03                                7   \n",
       "45                                0.03                                7   \n",
       "37                                0.03                               14   \n",
       "28                                0.03                               14   \n",
       "46                                0.03                               14   \n",
       "\n",
       "   param_lgbmclassifier__max_depth  mean_fit_time  rank_test_score  \n",
       "33                              -5       3.169050                1  \n",
       "42                              -1       3.131531                1  \n",
       "12                              -1       1.129297                3  \n",
       "3                               -5       1.123234                3  \n",
       "21                               5       1.098557                5  \n",
       "51                               5       3.212270                6  \n",
       "14                              -1       1.466649                7  \n",
       "5                               -5       1.440759                7  \n",
       "2                               -5       1.026436                9  \n",
       "11                              -1       0.998854                9  \n",
       "4                               -5       1.253935               11  \n",
       "13                              -1       1.344001               11  \n",
       "1                               -5       0.854937               13  \n",
       "10                              -1       0.900863               13  \n",
       "18                               5       0.775499               15  \n",
       "24                               5       3.236266               16  \n",
       "43                              -1       3.995424               17  \n",
       "34                              -5       3.881981               17  \n",
       "53                               5       4.074039               19  \n",
       "52                               5       3.568991               20  \n",
       "44                              -1       4.565319               21  \n",
       "35                              -5       4.922649               21  \n",
       "19                               5       0.719414               23  \n",
       "22                               5       1.146953               24  \n",
       "15                              -1       3.501403               25  \n",
       "6                               -5       4.092425               25  \n",
       "32                              -5       1.347827               27  \n",
       "41                              -1       1.301132               27  \n",
       "0                               -5       0.630462               29  \n",
       "9                               -1       0.705538               29  \n",
       "20                               5       0.806302               31  \n",
       "31                              -5       1.236416               32  \n",
       "40                              -1       1.143029               32  \n",
       "49                               5       1.188575               34  \n",
       "30                              -5       1.179529               35  \n",
       "39                              -1       0.981587               35  \n",
       "48                               5       0.975862               37  \n",
       "50                               5       1.094443               38  \n",
       "23                               5       1.192916               39  \n",
       "16                              -1       4.721466               40  \n",
       "7                               -5       4.264700               40  \n",
       "25                               5       3.686394               42  \n",
       "8                               -5       4.857510               43  \n",
       "17                              -1       5.575354               43  \n",
       "26                               5       4.472689               45  \n",
       "47                               5       0.730955               46  \n",
       "29                              -5       1.083035               47  \n",
       "38                              -1       0.779873               47  \n",
       "36                              -1       0.615090               49  \n",
       "27                              -5       0.675093               49  \n",
       "45                               5       0.602564               51  \n",
       "37                              -1       0.775058               52  \n",
       "28                              -5       0.740573               52  \n",
       "46                               5       0.685408               54  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lgbm = make_pipeline(\n",
    "    preprocessor, lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=42)\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"lgbmclassifier__num_leaves\": [7, 14, 21, ],\n",
    "    \"lgbmclassifier__learning_rate\": [0.1, 0.03],\n",
    "    \"lgbmclassifier__max_depth\": [-5,-1, 5],\n",
    "    \"lgbmclassifier__n_estimators\": [50, 100, 500],\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    pipe_lgbm,\n",
    "    param_grid,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    "    scoring = custom_scorer,\n",
    ")\n",
    "grid.fit(X_train, y_train);\n",
    "grid_df = pd.DataFrame(grid.cv_results_)[\n",
    "    [\n",
    "        \"mean_train_score\",\n",
    "        \"mean_test_score\",\n",
    "        \"param_lgbmclassifier__n_estimators\",\n",
    "        \"param_lgbmclassifier__learning_rate\",\n",
    "        \"param_lgbmclassifier__num_leaves\",\n",
    "        \"param_lgbmclassifier__max_depth\",\n",
    "        \"mean_fit_time\",\n",
    "        \"rank_test_score\",\n",
    "    ]\n",
    "]\n",
    "grid_df = grid_df.sort_values(by=\"mean_test_score\", ascending=False)\n",
    "grid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interpretation and feature importances <a name=\"1\"></a>\n",
    "<hr>\n",
    "rubric={points:15}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Use the methods we saw in class (e.g., `eli5`, `shap`) (or any other methods of your choice) to explain feature importances of one of the best performing models. Summarize your observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgbm_model = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.5559\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PAY_0\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0628\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PAY_2\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.98%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0374\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PAY_3\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.26%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0324\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                LIMIT_BAL\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.29%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0320\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                BILL_AMT1\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.43%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0297\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PAY_AMT2\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.45%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0293\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PAY_6\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.46%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0291\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PAY_AMT3\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.60%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0269\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PAY_AMT1\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.72%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0250\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PAY_5\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0203\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PAY_4\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.17%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0182\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PAY_AMT4\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.45%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0144\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                BILL_AMT2\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.46%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0143\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PAY_AMT5\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.72%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0109\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PAY_AMT6\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.78%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0102\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                AGE\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.82%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0097\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                BILL_AMT6\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.87%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0091\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                BILL_AMT4\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.98%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0079\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                BILL_AMT5\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.14%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0062\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                BILL_AMT3\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.14%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 12 more &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "binary_OHE = list(\n",
    "    best_lgbm_model.named_steps[\"columntransformer\"]\n",
    "    .named_transformers_[\"onehotencoder-1\"]\n",
    "    .get_feature_names(binary_features)\n",
    ")\n",
    "\n",
    "categorical_OHE = list(\n",
    "    best_lgbm_model.named_steps[\"columntransformer\"]\n",
    "    .named_transformers_[\"onehotencoder-2\"]\n",
    "    .get_feature_names(categorical_features)\n",
    ")\n",
    "\n",
    "feature_names = numeric_features + binary_OHE + categorical_OHE\n",
    "\n",
    "eli5.show_weights(\n",
    "    best_lgbm_model.named_steps[\"lgbmclassifier\"],\n",
    "    feature_names=feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our eli5, we can see that our most important features are the PAY, BILL_AMT, PAY_AMT columns with PAY_0 being our most important feature. This makes sense intuitively because whether or not a person pays in the first month could tell a lot about coming months. EDUCATION does not play as big of a role as I had suspected but this could be due to unknowns in the data in this column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results on the test set <a name=\"12\"></a>\n",
    "<hr>\n",
    "\n",
    "rubric={points:5}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try your best performing model on the test data and report test scores. \n",
    "2. Do the test scores agree with the validation scores from before? To what extent do you trust your results? Do you think you've had issues with optimization bias? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "score() got an unexpected keyword argument 'scoring'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m test\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m      2\u001b[0m pipe_test \u001b[38;5;241m=\u001b[39m make_pipeline(\n\u001b[0;32m      3\u001b[0m     preprocessor, best_lgbm_model)\n\u001b[1;32m----> 5\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mbest_lgbm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcustom_scorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# pd.DataFrame(cross_validate(pipe, X_train, y_train, return_train_score=True, scoring = custom_scorer )).mean()\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(test)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\utils\\metaestimators.py:113\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attr_err\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# lambda, but not partial, allows help() to work with update_wrapper\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;31mTypeError\u001b[0m: score() got an unexpected keyword argument 'scoring'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average f1 score on the test set: 0.685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "predictions = best_lgbm_model.predict(X_test)\n",
    "print(\n",
    "    \"Macro-average f1 score on the test set: %0.3f\"\n",
    "    % (f1_score(y_test, predictions, average=\"macro\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train_score on our test set is very similar to our f1 scores from before. This is an indicator that we are not having issues with optimization bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) 13. Explaining predictions \n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "1. Take one or two test predictions and explain them with SHAP force plots.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary of results <a name=\"13\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Report your final test score along with the metric you used. \n",
    "2. Write concluding remarks.\n",
    "3. Discuss other ideas that you did not try but could potentially improve the performance/interpretability . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final test score from applying the LGBM model with optimized hyperparameter is 0.685. This score is a Macro-average f1 score. I used the macro-average f1 score because there is class imbalance between defaulting payments and not defaulting payments. All our models performed better than our baseline DummyClassifier. RandomForest and LGBM performed similarly however it was found that RandomForest was likely to be overfitted as it had a high train_score compared to test_score when cross validating. This is why I used the LGBM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions \n",
    "\n",
    "**PLEASE READ:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from â€œ1â€ will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
